"""\
- bs_xml2jsonl で xml から jsonl へ変換された複数の jsonl ファイルを Elasticsearch にインポート可能なサイズまで分割して書き出す
"""
import argparse
import glob
import shutil
import sys
from multiprocessing import Pool
from pathlib import Path
from typing import List, Tuple

from pydantic import BaseModel

from ddbj_search_converter.config import (LOGGER, Config, get_config,
                                          set_logging_level)


class Args(BaseModel):
    jsonl_dir: Path
    output_dir: Path
    split_size: int = 30000  # 2G を超えるサイズの jsonl があったため 30000


def parse_args(args: List[str]) -> Tuple[Config, Args]:
    parser = argparse.ArgumentParser(
        description="Split jsonls file generated by bs_xml2jsonl with specified number of lines"
    )
    parser.add_argument(
        "jsonl_dir",
        help="Directory containing JSON-Lines files to split, which is generated by bs_xml2jsonl",
    )
    parser.add_argument(
        "output_dir",
        help="Directory to output the split JSON-Lines files",
    )
    parser.add_argument(
        "--overwrite",
        action="store_true",
        help="Overwrite the output directory if it already exists",
    )
    parser.add_argument(
        "--split-size",
        type=int,
        default=30000,
        help="Number of lines to split the JSON-Lines file (default: 30000)",
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug mode ",
    )

    parsed_args = parser.parse_args(args)

    # 優先順位: コマンドライン引数 > 環境変数 > デフォルト値 (config.py)
    config = get_config()
    if parsed_args.debug:
        config.debug = parsed_args.debug

    # Args の型変換と validation
    jsonl_dir = Path(parsed_args.jsonl_dir)
    if not jsonl_dir.exists():
        LOGGER.error("Input directory does not exist: %s", jsonl_dir)
        sys.exit(1)
    output_dir = Path(parsed_args.output_dir)
    if output_dir.exists():
        if parsed_args.overwrite:
            LOGGER.info("Output directory %s already exists, but will be overwritten", output_dir)
            shutil.rmtree(output_dir)
        else:
            LOGGER.error("Output directory already exists: %s", output_dir)
            sys.exit(1)
    output_dir.mkdir(parents=True, exist_ok=True)

    return (config, Args(
        jsonl_dir=jsonl_dir,
        output_dir=output_dir,
        split_size=parsed_args.split_size,
    ))


def split_files(jsonl_file: Path, output_dir: Path, split_size: int) -> None:
    """\
    jsonl ファイルを特定の行数のファイルに分割して出力する
    """
    with jsonl_file.open("r", encoding="utf-8") as f:
        file_count = 0
        lines = []
        for line_num, line in enumerate(f, start=1):
            lines.append(line)
            if line_num % split_size == 0:
                output_file = output_dir.joinpath(f"{jsonl_file.stem}_{file_count:06d}.jsonl")
                with output_file.open("w", encoding="utf-8") as f_out:
                    f_out.writelines(lines)
                file_count += 1
                lines.clear()

        # 残りのデータを出力
        if len(lines) > 0:
            output_file = output_dir.joinpath(f"{jsonl_file.stem}_{file_count:06d}.jsonl")
            with output_file.open("w", encoding="utf-8") as f_out:
                f_out.writelines(lines)


def main() -> None:
    config, args = parse_args(sys.argv[1:])
    set_logging_level(config.debug)
    LOGGER.info("Splitting JSON-Lines files in %s", args.jsonl_dir)
    LOGGER.info("Config: %s", config.model_dump())
    LOGGER.info("Args: %s", args.model_dump())

    input_files = [Path(f) for f in glob.glob(args.jsonl_dir.joinpath("*.jsonl").as_posix())]

    error_flag = False
    with Pool(config.process_pool_size) as p:
        try:
            p.starmap(split_files, [(f, args.output_dir, args.split_size) for f in input_files])
        except Exception as e:
            LOGGER.error("Error occurred while splitting files: %s", e)
            error_flag = True

    if error_flag:
        LOGGER.error("Failed to split JSON-Lines files, please check the log")
        sys.exit(1)

    LOGGER.info("Finished splitting JSON-Lines files")


if __name__ == "__main__":
    main()
